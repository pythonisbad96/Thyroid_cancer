# ğŸ§  ê°‘ìƒì„ ì•” ì§„ë‹¨ ë¶„ë¥˜ AI í”„ë¡œì íŠ¸

> ì˜ë£Œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°‘ìƒì„ ì•” ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.  
> ë‹¤ì–‘í•œ ëª¨ë¸ í•™ìŠµê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹, ì•™ìƒë¸”ì„ í†µí•´ F1-scoreë¥¼ ê·¹ëŒ€í™”í•˜ì˜€ìŠµë‹ˆë‹¤.

---

## ğŸ“Œ í”„ë¡œì íŠ¸ ê°œìš”

- **ëª©í‘œ**: í™˜ìì˜ ë‹¤ì–‘í•œ ì˜í•™ì  ì§€í‘œ(TSH, T3, T4 ë“±)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°‘ìƒì„ ì•” ì—¬ë¶€(`Cancer`)ë¥¼ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ ê°œë°œ
- **ì ‘ê·¼ ë°©ë²•**: ë°ì´í„° ì „ì²˜ë¦¬ â†’ ëª¨ë¸ë³„ íŠœë‹ â†’ ì•™ìƒë¸” â†’ Threshold ì¡°ì • â†’ ì„±ëŠ¥ ì‹œê°í™”
- **ì‚¬ìš© ê¸°ìˆ **: Python, Scikit-learn, XGBoost, CatBoost, LightGBM, Matplotlib

---

## ğŸ§¾ ë°ì´í„° ì •ë³´

- **Train.csv**: í•™ìŠµ ë° ê²€ì¦ ë°ì´í„°
- **Test.csv**: ì˜ˆì¸¡ìš© í…ŒìŠ¤íŠ¸ ë°ì´í„°
- **Sample_submission.csv**: ì œì¶œ í˜•ì‹ í…œí”Œë¦¿

---

## ğŸ”§ ì „ì²˜ë¦¬ ë° íŠ¹ì„± ì„ íƒ

- Label Encodingì„ í†µí•´ ë²”ì£¼í˜• ë³€ìˆ˜ ì²˜ë¦¬
- `Gender`, `Smoke`, `Weight_Risk`, `Diabetes`ëŠ” feature ì¤‘ìš”ë„ê°€ ë‚®ì•„ ì œê±°
- í•™ìŠµ/ê²€ì¦ ì„¸íŠ¸ë¥¼ 80:20 ë¹„ìœ¨ë¡œ ë¶„ë¦¬ (`stratify` ì ìš©)

---

## ğŸ¤– ëª¨ë¸ë§ ë° ì•™ìƒë¸”

### ğŸ¯ XGBoost (íŠœë‹ í¬í•¨)

```python
from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV

xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
param_grid = {
    'max_depth': [6],
    'learning_rate': [0.01, 0.1],
    'n_estimators': [100, 300],
    'scale_pos_weight': [1, 7.3, 15]
}
rs_xgb = RandomizedSearchCV(xgb, param_grid, scoring='f1', n_iter=10, cv=5)
rs_xgb.fit(X_train, y_train)
best_xgb = rs_xgb.best_estimator_
```

ğŸ“Œ **ì„¤ëª…**:  
XGBoost ëª¨ë¸ì— ëŒ€í•´ í•™ìŠµë¥ , íŠ¸ë¦¬ ê¹Šì´, í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ë“±ì„ ëœë¤ íƒìƒ‰ì„ í†µí•´ ìµœì í™”í•©ë‹ˆë‹¤.  
í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œëŠ” `scale_pos_weight`ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.

---

### ğŸ§© Soft Voting ì•™ìƒë¸”

```python
from sklearn.ensemble import VotingClassifier

voting_model = VotingClassifier(
    estimators=[('xgb', best_xgb), ('cat', best_cat), ('lgb', lgb_model)],
    voting='soft'
)
voting_model.fit(X_train, y_train)
```

ğŸ“Œ **ì„¤ëª…**:  
XGBoost, CatBoost, LightGBM ì„¸ ê°€ì§€ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‰ê· ë‚´ì–´ ë³´ë‹¤ ì•ˆì •ì ì¸ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ìœ ë„í•©ë‹ˆë‹¤.  
Soft Votingì€ `predict_proba()`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í™•ë¥  í‰ê· ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

---

### ğŸ” Threshold íŠœë‹

```python
from sklearn.metrics import f1_score
import numpy as np

proba = voting_model.predict_proba(X_val)[:, 1]
thresholds = np.arange(0.30, 0.71, 0.01)
f1_scores = [f1_score(y_val, proba > t) for t in thresholds]
best_thresh = thresholds[np.argmax(f1_scores)]
```

ğŸ“Œ **ì„¤ëª…**:  
ê¸°ë³¸ thresholdì¸ 0.5 ëŒ€ì‹ , F1-scoreê°€ ê°€ì¥ ë†’ì€ thresholdë¥¼ ì§ì ‘ íƒìƒ‰í•©ë‹ˆë‹¤.  
ì´ëŠ” í´ë˜ìŠ¤ ë¶ˆê· í˜• ìƒí™©ì—ì„œ Precision/Recall ê· í˜•ì„ ì¡ëŠ” ë° ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.

---

## ğŸ“ˆ ê²°ê³¼ ë° í‰ê°€

- **ìµœì¢… ê²€ì¦ ì •í™•ë„(Accuracy)**: 88.0%
- **í´ë˜ìŠ¤ 1 (ì•”)ì˜ F1-score**: 0.4688
- Confusion Matrix ë° Classification Report ë¶„ì„ í¬í•¨

### ğŸ¨ F1-score vs Threshold ì‹œê°í™”

![F1 vs Threshold](images/result.png)

> ìœ„ ê·¸ë˜í”„ëŠ” ë‹¤ì–‘í•œ thresholdì—ì„œì˜ F1-score ë³€í™”ë¥¼ ì‹œê°í™”í•œ ê²ƒìœ¼ë¡œ, ìµœì  ê¸°ì¤€ì ì„ ì„ íƒí•˜ëŠ” ë° í™œìš©ë©ë‹ˆë‹¤.

---

## ğŸ›  ì‚¬ìš©ëœ íŒ¨í‚¤ì§€

```text
pandas, scikit-learn, xgboost, catboost, lightgbm, matplotlib, numpy
```

---

## ğŸ“ í”„ë¡œì íŠ¸ íšŒê³ 

- í´ë˜ìŠ¤ ë¶ˆê· í˜• ëŒ€ì‘ ì „ëµ(scale_pos_weight, auto_class_weights)ì˜ íš¨ê³¼ ì²´ê°
- ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ ì•™ìƒë¸” + threshold ìµœì í™”ì˜ ì„±ëŠ¥ ìš°ìœ„ í™•ì¸
- í–¥í›„ SHAP í•´ì„ ë° SMOTE ê¸°ë°˜ ì¦ê°•ë„ ì‹¤í—˜í•  ì˜ˆì •

---

## ğŸ”— ì°¸ê³  ë§í¬

- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [CatBoost Documentation](https://catboost.ai/)
- [LightGBM Documentation](https://lightgbm.readthedocs.io/)

